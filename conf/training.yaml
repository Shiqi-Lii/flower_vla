defaults:
  - datamodule: oxe_data # uha_data
  - trainer: flower_trainer # flower_trainer # baseline_trainer mode_trainer

DATA_NAME: real_kitchen_droid_co_training # simpler_env #real_kitchen_droid_co_training

wandb:
  name: uha_${now:%H-%M-%S}_flower # uha
  group: joint_only_${now:%Y-%m-%d}
  project: OXE_pretraining 
  entity: bennoq
  mode: null

log_dir: logs/ 
obs_seq_len: 1 
goal_window_size: 1
seed: 42
obs_dim: 16 
goal_dim: 512
update_ema_every_n_steps: 1
decay: 0.999
rampup_ratio: 0.001 # 1-decay

# dataloader
goal_relabeling_strategy: null 
subsample_length: null 
skip_unlabeled: True
load_camera_views: ["primary" , "secondary"] # , "secondary", "wrist"] #, "secondary", "wrist"]

# model
obs_modalities: 'observation'
goal_modalities: 'task' # 'lang_text'
img_modalities: ["image_primary", "image_secondary"] # , "image_wrist", "image_secondary" ] #,  "image_wrist"]  # "image_secondary",
lang_modalities: ["language_instruction"]
target_modality: 'action'
drop_last: True
pin_memory: True
num_workers: 0

# pretrained_weights: /home/reuss/code/flower_vla_policy/horeka_trains/droid_joint_only/checkpoint_160000/model.safetensors
# often changed
gradient_accumulation_steps: 4 # effective batch_size = batch_size * grad_accumulation_steps
find_unused_parameters: True # is slower but can be required 
static_graph: False

act_dim: 8
max_proprio_dim: 16
act_seq_len: 20 # 1 # future states loaded in Uha
max_train_steps: 600000 # 100000 # 31000 # 23000 # 50000
max_eval_steps: 100 #0 #00 #250 # 50
eval_every_n_steps: 5000 #0 #00 #000 # 2000
save_every_n_steps: 20000 # 4000 # 2000 # 10000
shuffle_buffer_size: 300000 #000 # 1000 # 250000 # high influence on the consumed memory, but has to be big for IID to hold. Octo recommends >100k for large scale training (~1% of accumulated sample sized used in Octo)
batch_size: 180 # 1024 # 2048 # 128 # single GPU batchsize is automatically reduced in multi-GPU training: cfg.batch_size = cfg.batch_size / torch.cuda.device_count()
dataset_size_limit: null # null # 1000 # limits all datasets to the provided amount of trajectories

hydra:
  run:
    dir: ${log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
